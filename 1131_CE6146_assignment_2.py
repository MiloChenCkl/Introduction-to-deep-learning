# -*- coding: utf-8 -*-
"""1131_CE6146_Assignment_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Huwq6_gfuLg-DXsClXUMqG9mVtOoe8dq

# Task 1: Image Classification with CNN
"""

import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
from tensorflow.keras.datasets import cifar10, imdb
from tensorflow.keras import layers
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Input, LSTM, GRU, SimpleRNN, Dropout, Dense, Flatten, Dropout, BatchNormalization, Embedding
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.optimizers import RMSprop, Adam
from scikeras.wrappers import KerasRegressor
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, classification_report

""" 1. Data Preprocessing

*   Load the CIFAR-10 dataset and normalize the pixel values to the range [0, 1].
*   Apply simple data augmentation techniques, such as random flips or crops, to improve generalization.



"""

(X_train, y_train), (X_test, y_test) = cifar10.load_data()

X_train = X_train.astype('float32') / 255.0
X_test = X_test.astype('float32') / 255.0
y_train = to_categorical(y_train, 10)
y_test = to_categorical(y_test, 10)

X_train = np.array(X_train)
y_train = np.array(y_train)

train_datagen = ImageDataGenerator(
    rotation_range=15 ,
    horizontal_flip = True ,
    width_shift_range=0.2 ,
    height_shift_range=0.2 ,
    shear_range=0.2 ,
    zoom_range=0.2 ,
    data_format='channels_last' ,
    validation_split=0.2

)

train_generator = train_datagen.flow(X_train, y_train, batch_size=64, subset='training')
val_generator = train_datagen.flow(X_train, y_train, batch_size=64, subset='validation')

"""2.Model Implementation

*   Design a CNN architecture with at least two convolutional layers, followed by pooling layers and a fully connected output layer.
*   Choose appropriate activation functions and set the number of filters per convolutional layer.



3.Regularization Techniques

*   Use dropout or batch normalization to reduce overfitting.
*   Explain the regularization techniques you used and their purpose.

Regularization techniques : 選在每層layer中加上batch normalization，能有效的解決overfitting，也適合在CNN中使用
"""

model = tf.keras.Sequential([

    #input layer
    Input(shape=(32, 32, 3)),

    # First Convolutional Layer
    layers.Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(32, 32, 3)),
    BatchNormalization(),
    layers.MaxPooling2D(pool_size=(2, 2)),

    # Second Convolutional Layer
    layers.Conv2D(64, (3, 3), padding='same', activation='relu'),
    BatchNormalization(),
    layers.MaxPooling2D(pool_size=(2, 2)),

    # Third Convolutional Layer
    layers.Conv2D(128, (3, 3), padding='same', activation='relu'),
    BatchNormalization(),
    layers.MaxPooling2D(pool_size=(2, 2)),


    Flatten(),
    Dense(512, activation='relu'),
    Dropout(0.2),
    Dense(10, activation='softmax')
])

model.summary()

"""4. Training and Evaluation

*   Train the CNN with a cross-entropy loss function and an optimizer (e.g., Adam) while tracking training and validation loss and accuracy.
*   Evaluate the model on the test set, providing metrics such as accuracy and F1-score.


"""

model.compile(optimizer='adam',
        loss='categorical_crossentropy',
        metrics=['accuracy'])


early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)


history = model.fit(
        train_generator,
        validation_data=val_generator,
        epochs=50,
        validation_split=0.2,
        shuffle=True,
        callbacks=[early_stopping]
)

plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.show()

test_loss, test_acc = model.evaluate(X_test, y_test)
print(f"Test accuracy: {test_acc}")

y_pred = model.predict(X_test)
y_pred_classes = tf.argmax(y_pred, axis=1)
y_true_classes = tf.argmax(y_test, axis=1)

# Classification report
report = classification_report(y_true_classes, y_pred_classes, target_names=[f'Class {i}' for i in range(10)])
print("\nClassification Report:\n", report)

"""5. Report.

*   Model architecture:

  1.   Input layer : CIFAR-10 RGB image with dimensions 32x32 pixels and a shape of (32, 32, 3)
  2.   Convolution layer :
      1. First Convolutional Layer:
        * 32 filters of size (3, 3)
        * Padding: 'same' to retain input dimensions.
        * Activation: ReLU.
        * Batch normalization applied to stabilize training.
        * Followed by max pooling with a pool size of (2, 2) to reduce spatial dimensions.
      2. Second Convolutional Layer:

      * 64 filters of size (3, 3).
      * Padding: 'same'.
      * Activation: ReLU.
      * Batch normalization applied.
      * Followed by max pooling with a pool size of (2, 2).
      3. Third Convolutional Layer:

      * 128 filters of size (3, 3).
      * Padding: 'same'.
      * Activation: ReLU.
      * Batch normalization applied.
      * Followed by max pooling with a pool size of (2, 2).
  3.   Fully connected layer
      * Flatten: Converts the 3D feature maps into a 1D feature vector.
      * Dense Layer (512 units): Applies a fully connected layer with 512 units and ReLU activation to process the flattened feature vector.
      * Dropout: Adds a dropout layer to prevent overfitting by randomly dropping 20% of the connections during training.
      * Output Layer (10 units): Uses a fully connected layer with 10 units and softmax activation for classification into 10 categories.
    




*   Regularization techniques : earlystopping with patience=5, batch normalization and dropout
*   Hyperparameter choices : using adam as optimizer, batch size=64, epoch=50 and categorical_crossentropy as loss function
*   Learning curves observation : the accuracy and loss start fluctuating after around 10 epoch, seems like a little bit overfitting

# Task 2: Time Series Forecasting with LSTM and GRU

1.Data Preprocessing

* Normalize the temperature dataset to an appropriate range.
* Split the data into training and test sets, and use a sliding window (e.g., 10 days) to create input-output sequences.
"""

# Load the dataset
url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/daily-min-temperatures.csv'
df = pd.read_csv(url, parse_dates=['Date'], index_col='Date')

temperature = df['Temp'].values.reshape(-1, 1)

# Normalize the temperature data to the range [0, 1]
scaler = MinMaxScaler(feature_range=(0, 1))
temperature = scaler.fit_transform(temperature)

# Prepare the sliding window input-output sequences
def create_sequences(data, time_step):
    X, y = [], []
    for i in range(len(data) - time_step):
        X.append(data[i:(i + time_step)])
        y.append(data[i + time_step, 0])
    return np.array(X), np.array(y)

time_step = 10
X, y = create_sequences(temperature, time_step)

train_size = int(len(X) * 0.6)
val_size = int(len(X) * 0.2)
test_size = len(X) - train_size - val_size

X_train, X_val, X_test = X[:train_size], X[train_size:train_size + val_size], X[train_size + val_size:]
y_train, y_val, y_test = y[:train_size], y[train_size:train_size + val_size], y[train_size + val_size:]

print("X_train shape:", X_train.shape)
print("y_train shape:", y_train.shape)
print("X_val shape:", X_val.shape)
print("y_val shape:", y_val.shape)
print("X_test shape:", X_test.shape)
print("y_test shape:", y_test.shape)

"""2. Model Implementation (LSTM and GRU)

* Implement an LSTM and a GRU model, configuring hidden layer size and the number of layers.
* Describe the architecture and activation function choices for each model.
"""

def build_lstm(units=50, learning_rate=0.001):
    lstm_model = Sequential([
        Input(shape = (X_train.shape[1], 1)),
        LSTM(units=units, return_sequences=True),
        Dropout(0.2),
        LSTM(units=units, return_sequences=True),
        Dropout(0.2),
        LSTM(units=units, return_sequences=True),
        Dropout(0.2),
        LSTM(units=units),
        Dropout(0.2),
        Dense(units=1)
    ])

    lstm_model.compile(optimizer=RMSprop(learning_rate=learning_rate), loss='mean_squared_error')
    return lstm_model

def build_gru(units=50, learning_rate=0.001):
    gru_model = Sequential([
        Input(shape = (X_train.shape[1], 1)),
        GRU(units=units, return_sequences=True),
        Dropout(0.2),
        GRU(units=units, return_sequences=True),
        Dropout(0.2),
        GRU(units=units, return_sequences=True),
        Dropout(0.2),
        GRU(units=units),
        Dropout(0.2),
        Dense(units=1)
    ])
    gru_model.compile(optimizer=RMSprop(learning_rate=learning_rate), loss='mean_squared_error')
    return gru_model

"""3. Hyperparameter Tuning

* Select key hyperparameters (e.g., batch size, learning rate) and perform a simple random or grid search.
* Document the final hyperparameters used and explain the rationale behind the choices.

* learning rate : Sometimes 0.001 will overfitting, so I choose smaller learning rate as hyperparameters
* batch size :  Smaller batch sizes often improve generalization, while larger sizes stabilize gradients for faster training.
* units : Smaller units reduce complexity, while larger units allow the model to capture more detailed patterns.
"""

lstm_model = KerasRegressor(model=build_lstm, epochs=50, units=25, learning_rate=0.001)
gru_model = KerasRegressor(model=build_gru, epochs=50, units=25, learning_rate=0.001)

param_grid = {
    'learning_rate' : [0.0001, 0.0005, 0.001],
    'batch_size' : [32, 64],
    'units' : [25, 50, 100],
}

lstm_grid = GridSearchCV(estimator=lstm_model, param_grid=param_grid, cv=3 ,verbose=3)
lstm_grid_result = lstm_grid.fit(X_train, y_train)

gru_grid = GridSearchCV(estimator=gru_model, param_grid=param_grid, cv=3, verbose=3)
gru_grid_result = gru_grid.fit(X_train, y_train)


print("\nLSTM Best Results:")
print(f"Best Parameters: {lstm_grid_result.best_params_}")

print("\nGRU Best Results:")
print(f"Best Parameters: {gru_grid_result.best_params_}")

"""4. Evaluation and Comparison

* Train and test each model, evaluating with Mean Squared Error (MSE) and Mean Absolute Error (MAE).
* Compare the performance of the two models and analyze which model performs better for this task.

* LSTM vs GRU : The GRU outperforms the LSTM in this case, as it achieves lower MAE and MSE. While the LSTM architecture is designed to learn long-term dependencies effectively, the true values of the temperature fluctuate significantly over time. This makes GRU, with its simpler and more adaptive structure, better suited to capturing these short-term fluctuations, resulting in superior performance for this task.
"""

final_lstm_model = build_lstm(
    units=100, learning_rate=0.001
)
final_lstm_model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=1, validation_data=(X_val, y_val))
lstm_predictions = final_lstm_model.predict(X_test)

# Final GRU Model
final_gru_model = build_gru(
    units=100, learning_rate=0.001
)
final_gru_model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=1, validation_data=(X_val, y_val))
gru_predictions = final_gru_model.predict(X_test)

# Rescale predictions back to original scale
y_test_original = scaler.inverse_transform(y_test.reshape(-1, 1))
lstm_predictions_original = scaler.inverse_transform(lstm_predictions)
gru_predictions_original = scaler.inverse_transform(gru_predictions)

# Calculate Metrics
lstm_mse = mean_squared_error(y_test_original, lstm_predictions_original)
gru_mse = mean_squared_error(y_test_original, gru_predictions_original)

lstm_mae = mean_absolute_error(y_test_original, lstm_predictions_original)
gru_mae = mean_absolute_error(y_test_original, gru_predictions_original)

print("LSTM Performance:")
print(f"Mean Squared Error: {lstm_mse:.4f}")
print(f"Mean Absolute Error: {lstm_mae:.4f}")

print("\nGRU Performance:")
print(f"Mean Squared Error: {gru_mse:.4f}")
print(f"Mean Absolute Error: {gru_mae:.4f}")

# Plot the prediction
plt.figure(figsize=(20, 8))
plt.plot(y_test_original, label="True Values", color='blue', linewidth=2)
plt.plot(lstm_predictions_original, label="LSTM Predictions", color='red', linewidth=2)
plt.plot(gru_predictions_original, label="GRU Predictions", color='green', linewidth=2)
plt.title("True Values vs. LSTM Predictions vs. GRU Predictions", fontsize=16)
plt.xlabel("Time Steps", fontsize=14)
plt.ylabel("Temperature", fontsize=14)
plt.legend(fontsize=12)
plt.grid(alpha=0.3)

plt.show()

"""5. Report

* Models’ architectures : four layers of LSTM and GRU with dropout=0.2
* hyperparameter tuning process : using Grid search to identify the optimal set of hyperparameters, which batch size=32, learning rate=0.001, units=100
* Discuss the learning curves and final results for each model :
  * LSTM captures the overall trend but struggles to follow rapid fluctuations in the true values.
  * GRU provides better alignment with the true values, especially during rapid changes suggesting its ability to adapt to short-term variations.
  * GRU is the better choice due to its lower error metrics and better visual alignment with true values.

# Task 3: Text Classification with RNN, LSTM, and GRU

1. Data Preprocessing

* Tokenize and pad the sequences so that each input sequence has the same length.
* Use an embedding layer (e.g., word embeddings) to convert text data into a format suitable for the models.
"""

word_index = imdb.get_word_index()
vocab_size = len(word_index)
print(f"Vocabulary size: {vocab_size}")

num_words = 20000
(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=num_words)

X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)


max_length_train = max(len(sequence) for sequence in X_train)
max_length_val = max(len(sequence) for sequence in X_val)
max_length_test = max(len(sequence) for sequence in X_test)

print(f"Maximum length of sequences in training set: {max_length_train}")
print(f"Maximum length of sequences in validation set: {max_length_val}")
print(f"Maximum length of sequences in test set: {max_length_test}")

sequence_lengths = [len(sequence) for sequence in X_train]
plt.hist(sequence_lengths, bins=50)
plt.title("Distribution of Sequence Lengths")
plt.xlabel("Sequence Length")
plt.ylabel("Frequency")
plt.show()

max_length = 150

X_train = pad_sequences(X_train, maxlen=max_length, padding='post', truncating='post')
X_val = pad_sequences(X_val, maxlen=max_length, padding='post', truncating='post')
X_test = pad_sequences(X_test, maxlen=max_length, padding='post', truncating='post')

embedding_dim = 64
embedding_layer = Embedding(input_dim=num_words, output_dim=embedding_dim)

"""2. Model Implementation (LSTM and GRU) (10%)

* Implement RNN, LSTM, and GRU models for text classification.
* Adjust the hidden layer size and number of layers to ensure each model has a similar configuration, enabling fair comparison.
* Provide brief descriptions of each model’s structure and rationale for your design choices.
* Model’s structure: 64 units in each model with dropout=0.5 for preventing overfitting and learning rate=0.0001 to slower the training process
* Design choices : IMDB text classification is a relatively simple task, so a single-layer model was selected to avoid unnecessary complexity. Since the training process showed signs of overfitting, a larger dropout rate was applied to improve regularization. Additionally, a smaller learning rate was chosen to ensure smoother convergence and prevent large gradient updates, further addressing the overfitting issue.


"""

def build_rnn():
    model = Sequential([
        embedding_layer,
        SimpleRNN(64),
        Dropout(0.5),
        Dense(1, activation='sigmoid')
    ])
    optimizer = Adam(learning_rate=0.0001)
    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])
    return model

rnn_model = build_rnn()

def build_lstm():
    model = Sequential([
        embedding_layer,
        LSTM(64),
        Dropout(0.5),
        Dense(1, activation='sigmoid')
    ])
    optimizer = Adam(learning_rate=0.0001)
    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])
    return model

lstm_model = build_lstm()

def build_gru():
    model = Sequential([
        embedding_layer,
        GRU(64),
        Dropout(0.5),
        Dense(1, activation='sigmoid')
    ])
    optimizer = Adam(learning_rate=0.0001)
    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])
    return model

gru_model = build_gru()

batch_size = 64
epochs = 20

early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

rnn_history = rnn_model.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=batch_size, epochs=epochs, callbacks=[early_stopping])
lstm_history = lstm_model.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=batch_size, epochs=epochs, callbacks=[early_stopping])
gru_history = gru_model.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=batch_size, epochs=epochs, callbacks=[early_stopping])

"""3. Evaluation and Comparison

* Evaluate each model using accuracy, F1-score, and, if possible, additional relevant metrics like precision and recall.
* Conduct a deeper comparative analysis in your report, discussing the advantages and limitations of each model for text classification.
* Explain which model performs best under these conditions and why.

1. RNN:

  1. Accuracy: 72%
  2. F1-Score (Negative): 0.77, (Positive): 0.63
  3. Observations:
  High recall for the "Negative" class but poor recall for the "Positive" class.
Imbalanced performance between classes indicates RNN struggles to handle subtle differences in text sentiment.
  4. Advantages: Simple architecture, computationally efficient.
  5. Limitations: Struggles with long-term dependencies due to the vanishing gradient problem.
2. LSTM:

  1. Accuracy: 75%
  2. F1-Score (Negative): 0.79, (Positive): 0.69
  3. Observations:
  Improved accuracy and F1-score compared to RNN.
  Handles sequential dependencies better than RNN but still struggles with the "Positive" class (lower recall).
  4. Advantages: Handles long-term dependencies using its gating mechanism
  5. Limitations: Slightly overfits on the "Negative" class, leading to imbalanced performance.
3. GRU:

  1. Accuracy: 82%
  2. F1-Score (Negative): 0.83, (Positive): 0.82
  3. Observations:
  Best performance across all metrics, with balanced precision, recall, and F1-score.
  4. Advantages: Simpler than LSTM but still capable of handling sequential dependencies effectively.
"""

rnn_pred = rnn_model.predict(X_test)
lstm_pred = lstm_model.predict(X_test)
gru_pred = gru_model.predict(X_test)

rnn_pred_binary = (rnn_pred > 0.5).astype(int)
lstm_pred_binary = (lstm_pred > 0.5).astype(int)
gru_pred_binary = (gru_pred > 0.5).astype(int)

def print_classification_report(y_true, y_pred, model_name):
    print(f"--- {model_name} Classification Report ---")
    report = classification_report(y_true, y_pred, target_names=['Negative', 'Positive'])
    print(report)
    print("\n")

# Classification reports
print_classification_report(y_test, rnn_pred_binary, "RNN")
print_classification_report(y_test, lstm_pred_binary, "LSTM")
print_classification_report(y_test, gru_pred_binary, "GRU")

"""4. Report

Summarize your model choices and discuss the differences in performance between RNN, LSTM, and GRU. Highlight which model you would recommend for similar text classification tasks and provide supporting reasons.

Recommendation :  For simple text classification tasks with small datasets, the GRU model is recommended as it demonstrates better accuracy, F1-score, and balanced precision across both "Negative" and "Positive" classes.

"""