# -*- coding: utf-8 -*-
"""TheCaliforniaHousing_113522143.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mwXxF17lHHsRBwvbBXlAWHz3yeCnjTyB

Python code for loading and preprocessing the dataset

 讀取 Dataset, 接著後面處理 outlier 再進行 feature scaling
"""

from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split

california_housing = fetch_california_housing(as_frame=True)

california_housing.frame.info()
california_housing.frame.head()

X = california_housing.data
y = california_housing.target

"""Outlier Detection and Handling

  計算各個 feature 的 Z-Score, 當 feature 的值超過三個標準差則判斷為 outlier 並 removed, 接著將 Dataset 以 80% training data, 20% test data 的方式切分。


"""

from scipy import stats
import numpy as np
import pandas as pd


random.seed(42)
np.random.seed(42)


z_scores = np.abs(stats.zscore(X))
threshold = 3
outliers = (z_scores > threshold)

print(pd.DataFrame(outliers, columns=X.columns).sum())

X_filtered = X[(z_scores < threshold).all(axis=1)]
y_filtered = y[(z_scores < threshold).all(axis=1)]

X_train, X_test, y_train, y_test = train_test_split(X_filtered, y_filtered, test_size=0.2, random_state=42)

print(f"訓練集大小: {len(X_train)}")
print(f"測試集大小: {len(X_test)}")

X_train.head()

"""Feature scaling

利用 scikit learn 的 MinMaxScaler 做 feature scaling
"""

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()

X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

print(X_train)

"""Feature Selection

用 RFE based on Random Rorest Regression 篩選較相關的 feature, 這邊設定 feature 數量
為 5, 並將 ranking 與結果顯示出來

"""

from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import RFE

model = RandomForestRegressor()

rfe = RFE(estimator=model, n_features_to_select=5)
rfe.fit(X_train, y_train)

ranking = rfe.ranking_
selected_features = X.columns[rfe.support_]

print("Feature Rankings:", ranking)
print("Selected Features:", selected_features)

"""Implementing chosen neural network architecture

"""

import torch
import torch.nn as nn
import tensorflow as tf
from tensorflow.keras import layers
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

torch.manual_seed(42)

model = tf.keras.Sequential([
    layers.Input(shape=(8,) ),
    layers.Dense(50, activation='relu'),
    layers.Dense(50, activation='relu'),
    layers.Dense(1, activation='linear')
])

model.compile(optimizer='SGD', loss='mean_squared_error')

history = model.fit(X_train, y_train, epochs=50, batch_size=64, validation_split=0.2)

y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
print(f"Mean Squared Error (MSE): {mse}")

"""Training and validation curves plotted over epochs


"""

import matplotlib.pyplot as plt

# Plot the training and validation loss
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss Over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

"""Hyperparameter Tuning

用 Grid search 找最佳 hyperparameters
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install scikeras

from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import train_test_split
from scikeras.wrappers import KerasRegressor
from sklearn.model_selection import GridSearchCV

# Define the function to create the model (required for KerasClassifier)
def create_model(optimizer='sgd', neurons=5, activation='relu'):
    model = tf.keras.Sequential([
        layers.Input(shape=(8,)),
        layers.Dense( neurons, activation=activation),
        layers.Dense( neurons, activation=activation),
        layers.Dense(1, activation='linear')
    ])

    model.compile(optimizer=optimizer, loss='mean_squared_error')

    return model


model = KerasRegressor(model=create_model, epochs=50, batch_size=64, verbose=2)

param_grid = {
    'model__optimizer': ['sgd','adam','rmsprop'],
    'model__neurons': [5, 10, 25],
    'model__activation': ['relu', 'tanh'],
    'batch_size': [64],
    'epochs': [50]
}

grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring='neg_mean_squared_error', cv=3, n_jobs = -1)
grid_result = grid.fit(X_train, y_train)

print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))

"""Apply regularization techniques (e.g., L1, L2, Dropout) to mitigate overfitting.

基本上只看得出來加上 regularization 後, 不論 L1, L2, 或 Dropout, training loss 和 validation loss 的下降速度降低, L1 下降速度最慢, 再來是 Dropout, 最後是 L2.
"""

from keras import regularizers

def create_and_train_model(regularizer=None, dropout_rate=None):
    model = tf.keras.Sequential()
    model.add(layers.Input(shape=(8,)))

    if regularizer:
        model.add(layers.Dense(25, activation='relu', kernel_regularizer=regularizer))
        model.add(layers.Dense(25, activation='relu', kernel_regularizer=regularizer))
    elif dropout_rate:
        model.add(layers.Dense(25, activation='relu'))
        model.add(layers.Dropout(dropout_rate))
        model.add(layers.Dense(25, activation='relu'))
        model.add(layers.Dropout(dropout_rate))
    else:
        model.add(layers.Dense(25, activation='relu'))
        model.add(layers.Dense(25, activation='relu'))


    model.add(layers.Dense(1, activation='linear'))

    model.compile(optimizer='SGD', loss='mean_squared_error')

    history = model.fit(X_train, y_train, epochs=50, batch_size=64, validation_split=0.2, verbose=0)

    return history


history_l1 = create_and_train_model(regularizer=regularizers.l1(0.001))
history_l2 = create_and_train_model(regularizer=regularizers.l2(0.001))
history_dropout = create_and_train_model(dropout_rate=0.3)
history_no_reg = create_and_train_model()

# Plot the training and validation loss for each model
plt.figure(figsize=(12, 8))

# L2 Regularization
plt.plot(history_l1.history['loss'], label='L1 Training Loss', linestyle='-')
plt.plot(history_l1.history['val_loss'], label='L1 Validation Loss', linestyle='--')

# L1 Regularization
plt.plot(history_l2.history['loss'], label='L2 Training Loss', linestyle='-')
plt.plot(history_l2.history['val_loss'], label='L2 Validation Loss', linestyle='--')

# Dropout Regularization
plt.plot(history_dropout.history['loss'], label='Dropout Training Loss', linestyle='-')
plt.plot(history_dropout.history['val_loss'], label='Dropout Validation Loss', linestyle='--')

# No Regularization
plt.plot(history_no_reg.history['loss'], label='No Regularization Training Loss', linestyle='-')
plt.plot(history_no_reg.history['val_loss'], label='No Regularization Validation Loss', linestyle='--')

# Customizations
plt.title('Training and Validation Loss with Different Regularization Techniques')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()

"""Use an adaptive learning rate optimizer (e.g., Adam, RMSprop) instead of a fixed learning rate SGD to improve the training process."""

model = tf.keras.Sequential([
    layers.Input(shape=(8,)),
    layers.Dense(25, activation='relu', kernel_regularizer=regularizers.l2(0.001)),
    layers.Dense(25, activation='relu', kernel_regularizer=regularizers.l2(0.001)),
    layers.Dense(1, activation='linear')
])

# Use Adam optimizer
model.compile(optimizer='adam', loss='mean_squared_error')

history = model.fit(X_train, y_train, epochs=50, batch_size=64, validation_split=0.2)


# Plot the training and validation loss
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss Over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

"""Evaluation metrics on the test set."""

y_pred = model.predict(X_test)

# Calculate Mean Squared Error (MSE)
mse = mean_squared_error(y_test, y_pred)
# Calculate Mean Absolute Error (MAE)
mae = mean_absolute_error(y_test, y_pred)
# Calculate R² score
r2 = r2_score(y_test, y_pred)

# Print the results
print(f"Mean Squared Error (MSE): {mse}")
print(f"Mean Absolute Error (MAE): {mae}")
print(f"R² Score: {r2}")

"""A brief report in Google Colab

Model architecture, hyperparameter choices: 在這次作業的 Model 中, 選用兩層 hidden layers 各 25 units 進行訓練。 Hyperparameters 則是使用上面 grid search 的最佳結果, 主要使用 relu 作為 activation function, adam 為 model 的 optimizer。在最終 model 預測結果的 Mean square error 約為 0.346, 也列了 Mean absolute error 和 R squared 做為參考。

Analysis of learning curves: 在 learning curves 方面, 可能因為 model 沒有使用過於複雜的架構, 所以 training loss and validation loss 在 training 的最後幾次 epoch 都趨於穩定, 沒有 overfitting 或 underfitting 的跡象, 大部分時候在前面幾個 epoch 就會將 loss 快速下降, validation loss 在某些時候會上下輕微震盪, 但應該是不影響 training 結果。

Task 2: Transform the Regression Task into a Classification Problem

Transform the regression task into a classification problem by converting the target variable (median house value) into discrete categories (e.g., “Low”, “Medium”, “High”). Train a neural network to perform this classification. Provide a rationale for your choice of categorization, explaining why it is meaningful and how it benefits this problem.
"""

from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.utils import to_categorical
from sklearn.metrics import accuracy_score



low_threshold = np.percentile(y_train, 33)
high_threshold = np.percentile(y_train, 66)

def categorize(value):
    if value <= low_threshold:
        return 'Low'
    elif value <= high_threshold:
        return 'Medium'
    else:
        return 'High'


y_train_categorical = np.array([categorize(value) for value in y_train])
y_test_categorical = np.array([categorize(value) for value in y_test])


label_encoder = LabelEncoder()
y_train_encoded_labels = label_encoder.fit_transform(y_train_categorical)
y_test_encoded_labels = label_encoder.transform(y_test_categorical)

y_train_encoded = to_categorical(y_train_encoded_labels, num_classes=3)

model = tf.keras.Sequential([
    layers.Input(shape=(8,)),
    layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.001)),
    layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001)),
    layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001)),
    layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.001)),
    layers.Dense(3, activation='softmax')
])


model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

history = model.fit(X_train, y_train_encoded, epochs=50, batch_size=64, validation_split=0.2)

y_pred_probs = model.predict(X_test)

y_pred_labels = np.argmax(y_pred_probs, axis=1)

accuracy = accuracy_score(y_test_encoded_labels, y_pred_labels)
print(f"Test Accuracy: {accuracy:.4f}")

"""Choice of categorization and benefits: 用百分比的方式將 MedHouseVal 分成低, 中, 高 三種結果, 這種方式相較於直接用 MedHouseVal 去訓練模型可以得到較直觀的訓練結果, 可以知道預測結果的準確率, 降低異常值對 loss 的影響, 實用性上能讓非專業人士對不同房屋價錢有更簡單的了解。

"""